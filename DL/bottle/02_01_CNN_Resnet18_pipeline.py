import torch
import random
import numpy as np
from torch import nn, optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
from pathlib import Path
from PIL import Image

SEED = 4
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# ğŸ“Œ í´ë˜ìŠ¤ ì •ì˜
CLASS_NAMES = ['bad-broken_large', 'bad-broken_small', 'bad-contamination', 'bottle-good']

# ğŸ“ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
root = Path(__file__).parent.resolve()
base_dir = root / "dataset/fixed_data_split"
train_dir, val_dir, test_dir = base_dir / "train", base_dir / "val", base_dir / "test"

# ğŸ§± Letterbox ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ í•¨ìˆ˜
def letterbox_image(image, target_size=(256, 256)):
    iw, ih = image.size
    w, h = target_size
    scale = min(w / iw, h / ih)
    nw, nh = int(iw * scale), int(ih * scale)
    image_resized = image.resize((nw, nh), Image.BICUBIC)
    new_image = Image.new('RGB', target_size, (128, 128, 128))
    new_image.paste(image_resized, ((w - nw) // 2, (h - nh) // 2))
    return new_image

# ğŸ” ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
transform = transforms.Compose([
    transforms.Lambda(lambda img: letterbox_image(img, (256, 256))),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# ğŸ“¦ ë°ì´í„°ì…‹ & ë¡œë”
train_dataset = datasets.ImageFolder(train_dir, transform=transform)
val_dataset = datasets.ImageFolder(val_dir, transform=transform)
test_dataset = datasets.ImageFolder(test_dir, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# ğŸ“¡ ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("ğŸ“Œ Using device:", device)

# ğŸ§  ëª¨ë¸ ì •ì˜ ë° ì¶œë ¥ì¸µ ìˆ˜ì •
model = models.resnet18(pretrained=True)
model.fc = nn.Sequential(
    nn.Dropout(0.3),
    nn.Linear(model.fc.in_features, len(CLASS_NAMES))
)
model = model.to(device)

# ğŸ¯ ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)

# â± í•˜ì´í¼íŒŒë¼ë¯¸í„°
EPOCHS = 100
patience = 15
best_val_acc = 0.0
best_val_loss = float('inf')
best_all_val_acc = 0.0
best_all_val_loss = float('inf')
best_train_acc = 0.0
best_train_loss = float('inf')

# ğŸ“ ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
model_dir = root / "model"
model_dir.mkdir(parents=True, exist_ok=True)

# ğŸš€ í•™ìŠµ ë£¨í”„
patience_counter = 0  # Early Stoppingì„ ìœ„í•œ ì¹´ìš´í„°

for epoch in range(EPOCHS):
    # ğŸ”§ í›ˆë ¨
    model.train()
    train_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    train_acc = correct / total
    train_loss /= len(train_loader)

    # ğŸ§ª ê²€ì¦
    model.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            val_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()

    val_acc = val_correct / val_total
    val_loss /= len(val_loader)

    # ğŸ“Š ì„±ëŠ¥ ì¶œë ¥
    print(f"\nğŸ“… Epoch {epoch+1}")
    print(f"ğŸ”§ Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}")
    print(f"ğŸ§ª Validation Accuracy: {val_acc:.4f}, Loss: {val_loss:.4f}")

    # ğŸ’¾ best_val_cnn ì €ì¥ ì¡°ê±´: validation ì •í™•ë„ & ì†ì‹¤ì´ ê°œì„ ëœ ê²½ìš°
    if (
        val_acc > best_val_acc or (val_acc == best_val_acc and val_loss < best_val_loss)
    ):
        best_val_acc = val_acc
        best_val_loss = val_loss
        torch.save(model.state_dict(), model_dir / "Resnet18/best_val.pt")
        print(f"âœ… Saved best_val.pt (Epoch {epoch+1})")
        patience_counter = 0  # ì„±ëŠ¥ í–¥ìƒë˜ë©´ ì¹´ìš´í„° ì´ˆê¸°í™”
    else:
        patience_counter += 1  # ì„±ëŠ¥ í–¥ìƒ ì—†ìœ¼ë©´ ì¹´ìš´í„° ì¦ê°€

    # ğŸ’¾ best_all_cnn ì €ì¥ ì¡°ê±´: 4ê°œ ì§€í‘œ ëª¨ë‘ ê°œì„ ëœ ê²½ìš°ë§Œ ì €ì¥
    if (
        (val_acc > best_all_val_acc or (val_acc == best_all_val_acc and val_loss < best_all_val_loss)) and
        (train_acc > best_train_acc or (train_acc == best_train_acc and train_loss < best_train_loss))
    ):
        best_all_val_acc = val_acc
        best_all_val_loss = val_loss
        best_train_acc = train_acc
        best_train_loss = train_loss
        torch.save(model.state_dict(), model_dir / "Resnet18/best_all.pt")
        print(f"âœ… Saved best_all.pt (Epoch {epoch+1}) by relaxed all-metric condition")

    # ğŸ“… Early Stopping ì¡°ê±´: patienceë¥¼ ì´ˆê³¼í•œ ê²½ìš° í•™ìŠµ ì¤‘ë‹¨
    if patience_counter >= patience:
        print(f"ğŸš¨ Early stopping at Epoch {epoch+1} due to no improvement")
        break

    scheduler.step()

# ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥
torch.save(model.state_dict(), model_dir / "Resnet18/last_model.pt")
print("ğŸ“¦ Final model saved as last_model.pt")